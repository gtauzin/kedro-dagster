# Technical Documentation

## Project Structure and Configuration

Kedro-Dagster expects a standard Kedro project structure. The main configuration file for Dagster integration is `dagster.yml`, located in your Kedro project's `conf/<ENV_NAME>/` directory.

### dagster.yml

This YAML file defines jobs, executors, and schedules for your project. Example:

```yaml
schedules:
  my_job_schedule:
    cron_schedule: "0 0 * * *"

executors:
  my_executor:
    multiprocess:
      max_concurrent: 2

jobs:
  my_job:
    pipeline:
      pipeline_name: __default__
    executor: my_executor
    schedule: my_job_schedule
```

- **jobs**: Map Kedro pipelines to Dagster jobs, with optional filtering.
- **executors**: Define how jobs are executed (in-process, multiprocess, k8s, etc).
- **schedules**: Set up cron-based or custom schedules for jobs.

### Customizing Schedules

TODO

### Customizing Executors

Kedro-Dagster supports several executor types for running your jobs, such as in-process, multiprocess, Dask, Docker, Celery, and Kubernetes. You can customize executor options in your `dagster.yml` file under the `executors` section.

#### Example: Custom Multiprocess Executor

```yaml
executors:
  my_multiprocess_executor:
    multiprocess:
      max_concurrent: 4
```

#### Example: Custom Docker Executor

```yaml
executors:
  my_docker_executor:
    docker_executor:
      image: my-custom-image:latest
      env_vars:
        - ENV=prod
      networks:
        - my_network
```

#### Example: Custom Kubernetes Executor

```yaml
executors:
  my_k8s_executor:
    k8s_job_executor:
      job_namespace: my-namespace
      image_pull_policy: Always
      env_vars:
        - ENV=prod
      resources:
        limits:
          cpu: "2"
          memory: "4Gi"
```

### Customizing Jobs

TODO explain that pipeline filtering is supported
TODO specify that each job can be associated with a pre-defined executor and/or schedule

### definitions.py

The `definitions.py` file is auto-generated by the plugin and serves as the main entry point for Dagster to discover all translated Kedro objects. It contains the Dagster `Definitions` object, which registers all jobs, assets, resources, and schedules derived from your Kedro project.

- **Purpose:** Allows Dagster to load your Kedro pipelines, datasets, and configuration as Dagster jobs, assets, and resources.
- **Location:** Placed at the root of your Kedro project.
- **Regeneration:** Re-run `kedro dagster init` to update this file if your Kedro project structure changes.

Typical contents include:

- All Dagster jobs corresponding to Kedro pipelines.
- Asset definitions for datasets.
- Resource definitions for IO managers and configuration.
- Schedule and sensor definitions if configured in `dagster.yml`.

You should not manually edit `definitions.py`; instead, update your Kedro project or configuration and regenerate as needed.

## Kedro-Dagster Concept Mapping

Kedro-Dagster translates core Kedro concepts into their Dagster equivalents. Understanding this mapping helps you reason about how your Kedro project appears and behaves in Dagster.

| Kedro Concept   | Dagster Concept      | Description |
|-----------------|----------------------|-------------|
| **Node**        | Op,&nbsp;Asset            | Each Kedro node becomes a Dagster op. Node parameters are passed as config. |
| **Pipeline**    | Job                  | Each Kedro pipeline is translated into a Dagster job. Jobs can be filtered and scheduled. |
| **Dataset**     | Asset,&nbsp;IO&nbsp;Manager    | Kedro datasets become Dagster assets (if named as such) and are managed by custom IO managers. |
| **Hooks**       | Hooks,&nbsp;Sensors       | Kedro hooks (before/after pipeline run, on error, etc.) are executed at the appropriate points in the Dagster job lifecycle. |
| **Parameters**  | Config,&nbsp;Resources    | Kedro parameters are passed as Dagster config or resources. |
| **Logging**     | Logger               | Kedro logging is integrated with Dagster's logging system. |

### Catalog Integration

Kedro-Dagster translates Kedro datasets into Dagster assets and IO managers. This allows you to use Kedro's Data Catalog with Dagster's asset materialization and IO management features.

- **External assets**: Datasets not generated by Dagster are registered as external assets.
- **IO Managers**: Custom IO managers are created for each dataset type, supporting both in-memory and persistent datasets.

See the API reference for `CatalogTranslator` for more details.

### Node Translation

Kedro nodes are translated into Dagster ops and assets. Each node becomes a Dagster op, and, additionally, nodes that returns outputs are mapped to Dagster multi-assets.

- **Ops**: Each Kedro node is mapped to a Dagster op.
- **Assets**: Outputs that are asset-named are registered as Dagster assets.
- **Parameters**: Node parameters are passed as Dagster config.

See the API reference for `NodeTranslator` for more details.


### Pipeline Translation

Kedro pipelines are translated into Dagster jobs. Each job can be filtered, scheduled, and assigned an executor via configuration.

- **Jobs**: Each pipeline is mapped to a Dagster job.
- **Filtering**: Jobs can filter nodes, tags, and inputs/outputs.
- **Hooks**: Before/after pipeline run hooks are supported.

See the API reference for `PipelineTranslator` for more details.

### Hook Integration

Kedro-Dagster preserves Kedro hooks in the Dagster context. Supported hooks include:

- `after_context_created`
- `before_pipeline_run`
- `after_pipeline_run`
- `on_pipeline_error`

Hooks are executed at the appropriate points in the Dagster job lifecycle. See the API reference for details on hook translation.
